{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3619fbe-5fda-43e3-8cec-c1a0e7da7e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c97e6d-cc74-46b3-a3c4-c8e053f565a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create schema if not exists telecom_catalog_assign.landing_zone;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef79375-afc3-4359-b7dc-858ddcf96228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "for folder in [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\"\n",
    "]:\n",
    "    dbutils.fs.mkdirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "779a127f-0500-498f-98b0-54c7e0491539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):\n",
    "a. Volume vs DBFS/FileStore\n",
    "b. Why production teams prefer Volumes for regulated data\n",
    "\n",
    "****** NEED TO WORK LATER *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbbf2fa-fc05-42ee-a2ec-a569ecfd1b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "customer_csv = \"\"\"101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "\"\"\"\n",
    "\n",
    "usage_tsv = \"\"\"customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "\"\"\"\n",
    "\n",
    "tower_logs_region1 = \"\"\"event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d1c696-fe03-4c1f-938b-d4fe6b76bcd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", customer_csv, True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", usage_tsv,True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\", tower_logs_region1,True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\", tower_logs_region1,True)\n",
    "\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6789ea85-a67e-4b2e-8126-ff969b440d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# 3.1 Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup\n",
    "spark.read.csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"],header=True,inferSchema=True,sep=\"|\",pathGlobFilter=\"*.csv\",recursiveFileLookup=True).show()\n",
    "\n",
    "#3.2 Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "spark.read.option(\"header\",\"True\").option(\"inferSchema\",\"True\").option(\"sep\",\"|\").option(\"pathGlobFilter\",\"*.csv\").option(\"recursiveFileLookup\",\"True\").format(\"CSV\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d496578-47c5-4fe5-b35d-c0968d209a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "******** Compare the outputs and understand when each should be used. ( Need To Work )******** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddadfd19-e6c2-418e-9819-9c54ae6474d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Customer\n",
    "custdf =spark.read.options(header=\"False\",inferSchema=\"True\",sep=\",\",pathGlobFilter=\"*.csv\",recursiveFileLookup=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "custdf.show()\n",
    "\n",
    "# Usage\n",
    "usagedf=  spark.read.options(header=\"True\",inferSchema=\"True\",sep=\"\\t\",pathGlobFilter=\"*.csv\",recursiveFileLookup=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "usagedf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e38325a-2f58-4c43-9b7f-70b02c68c621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####  Write a note on What changed when we use header or inferSchema with true/false?\n",
    "\n",
    "<b>header=\"True\" </b> :  It will display the data with user given header <br>\n",
    "<b>header=\"False\" </b>: It will not display the data with user given header. But defaultly it will show the column name like c0, c1, c2 ....  <br>\n",
    "<b>inferSchema=\"True\" </b>: It will analyze/Scan the column data and it will show the data type of column.   <br>\n",
    "<b>inferSchema=\"False\" </b>: It consider and show all the column data type as string (default) <br><br>\n",
    "\n",
    "\n",
    "#### How schema inference handled “abc” in age?\n",
    "Age contains both integer and string value so it is considerign the age column as string \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7bf9ef-760d-4621-89d3-ab7455afded7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,TimestampType\n",
    "#5.1\n",
    "custdf.toDF(\"Id\",\"Name\",\"Age\",\"Location\",\"SimType\").show()\n",
    "\n",
    "#5.2\n",
    "custom_schema = StructType([StructField(\"cust_id\",IntegerType(),False),StructField(\"voi_mins\",IntegerType(),True),StructField(\"data_mb\",IntegerType(),True),StructField(\"sms_count\",IntegerType(),True)])\n",
    "\n",
    "custom_df1=spark.read.schema(custom_schema).options(header=\"True\", sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "custom_df1.show()\n",
    "\n",
    "custom_df1.printSchema()\n",
    "\n",
    "#5.3\n",
    "tower_schema = StructType([\n",
    "    StructField(\"eve_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "tower_df1=spark.read.schema(tower_schema).options(header=\"True\", sep=\"|\" ,pathGlobFilter=\"*.csv\",recursiveFileLookup=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "tower_df1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. More to come (stay motivated)...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784801e4-21df-4b4f-88c8-2d988f5b5e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d88cfa-4ca8-4899-a5c2-7c77c436c65e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "#1 : Write customer data into CSV format using overwrite mode\n",
    "read_cust = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\") \n",
    "write_cust = read_cust.write.options(header='true').mode(\"overwrite\").csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "\n",
    "#2 : Write usage data into CSV format using append mode\n",
    "read_usage = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")  \n",
    "append_usage = read_usage.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usageout/\",header='true',mode='append')\n",
    "display(append_usage) \n",
    "\n",
    "#3 : Write tower data into CSV format with header enabled and custom separator (|)\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\") \n",
    "append_usage = read_tower_df.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towerout/\",header='true',mode='append')\n",
    "\n",
    "#4 : Read the tower data in a dataframe and show only 5 rows.\n",
    "\n",
    "read_tower_df.show(1)\n",
    "\n",
    "#5 : Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "#Yes, we can see the data in notepad++\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e0c53f-5bc9-4091-a1b4-5bce363f0ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_cust = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\") \n",
    "write_cust = read_cust.write.options(header='true').mode(\"overwrite\").csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\"))\n",
    "\n",
    "files = [\n",
    "    f for f in dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "    if f.name.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "latest_csv = sorted(\n",
    "    files,\n",
    "    key=lambda x: x.modificationTime,\n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "print(latest_csv.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8513626f-7308-45ab-8732-531b0d204b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacc91e7-baf4-4688-8059-a4b7a3ea03ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 : Write customer data into JSON format using overwrite mode\n",
    "read_customer_df = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\") \n",
    "write_customer_json_df = read_customer_df.write.mode(\"append\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jsonout/\")\n",
    "\n",
    "#2 : Write usage data into JSON format using append mode and snappy compression format\n",
    "read_usage = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\") \n",
    "read_usage.write.mode(\"append\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/jsonout/\",compression=\"snappy\")\n",
    "\n",
    "#3 : Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\") \n",
    "write_tower_df = read_tower_df.write.mode(\"ignore\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/jsonout/\")\n",
    "display(write_tower_df)\n",
    "\n",
    "#4 : Read the tower data in a dataframe and show only 5 rows.\n",
    "read_tower_df.show(1)\n",
    "\n",
    "#5 : Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "#Yes, we can see the data in notepad++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de7cc064-ddc7-41eb-ba75-24b7ccdd84ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698c179d-c29a-475e-9208-f7fcba1d7f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 : Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "read_customer_df = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\") \n",
    "write_customer_json_df = read_customer_df.write.mode(\"overwrite\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout/\")\n",
    "\n",
    "#2:Write usage data into Parquet format using error mode\n",
    "read_usage = spark.read.options(header=\"False\",sep=\"\\t\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\") \n",
    "read_usage.write.mode(\"error\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout/\")\n",
    "\n",
    "#3 : Write tower data into Parquet format with gzip compression option\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\") \n",
    "write_tower_df = read_tower_df.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/parquetout/\",compression=\"gzip\")\n",
    "display(write_tower_df)\n",
    "\n",
    "#4 : Read the tower data in a dataframe and show only 5 rows.\n",
    "read_tower_df.show(1)\n",
    "\n",
    "#5 : Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "#Yes, we can see the data in notepad++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2699af87-4c65-4468-95ae-fda65dbfdf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79c877e-e448-4bd1-bccd-4543cd6132ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: Write customer data into ORC format using overwrite mode\n",
    "read_customer_df = spark.read.options(header=\"False\",sep=\",\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\") \n",
    "write_customer_orc_df = read_customer_df.write.mode(\"overwrite\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout/\") \n",
    "\n",
    "#2 : Write usage data into ORC format using append mode\n",
    "read_usage = spark.read.options(header=\"False\",sep=\"\\t\").format(\"csv\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\") \n",
    "read_usage.write.mode(\"append\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout/\")\n",
    "\n",
    "#3 : Write tower data into ORC format and see the output file structure\n",
    "read_tower_df = spark.read.options(header='true',sep='\\t',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\") \n",
    "write_tower_df = read_tower_df.write.mode(\"append\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/orcout/\")\n",
    "display(write_tower_df) \n",
    "\n",
    "#4 : Read the usage data in a dataframe and show only 5 rows.\n",
    "read_usage.show(5)\n",
    "\n",
    "#5 : Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "#Yes, we can see the data in notepad++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d69c0a-d3c2-4e34-b6cd-d5f526a2a5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f7d631-4154-4c96-99f7-251067f8fe0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Write customer data into Delta format using overwrite mode\n",
    "read_delta= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\")\n",
    "Write_delta=read_delta.write.mode(\"overwrite\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltaout/\")\n",
    "display(read_delta)\n",
    "display(Write_delta)\n",
    "##Write usage data into Delta format using append mode\n",
    "read_delta_usage= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "Write_delta_usage=read_delta.write.mode(\"append\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout/\")\n",
    "##Write tower data into Delta format and see the output file structure\n",
    "read_delta_tower= spark.read.options(header=True).options(sep='|').options(pathgolbfilter=\".csv\").options(recursivefilelookup=\"true\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\")\n",
    "write_delta_tower=read_delta_tower.write.mode(\"overwrite\").option(\"compression\",\"gzip\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/deltaout\")\n",
    "display(read_delta_tower)\n",
    "display(write_delta_tower)\n",
    "\n",
    "#Read the usage data in a dataframe and show only 5 rows.\n",
    "display(read_delta_usage.limit(5))\n",
    "\n",
    "#5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "'''I downloaded all the files into local machine from the catolog volume location and I was unable to read the data because it is compressed and stored in delta format (Internally as Parquet format)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855fa327-8e39-46ef-8a7a-c2b8eca13ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd03ada4-c3a1-45ce-9453-2edd8d8bb39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "##Write customer data using saveAsTable() as a managed table\n",
    "dlt_tb=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "dlt_tb.write.saveAsTable(\"telecom_catalog_assign.landing_zone.dlt_tb\")\n",
    "dlt_tb.show()\n",
    "##Write usage data using saveAsTable() with overwrite mode\n",
    "dlt_tb_usage=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "dlt_tb_usage.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.landing_zone.dlt_tb_usage\")\n",
    "dlt_tb_usage.show()\n",
    "\n",
    "## Drop the managed table and verify data removal\n",
    "dlt_tb=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "dlt_tb.write.saveAsTable(\"telecom_catalog_assign.landing_zone.dlt_tb1\")\n",
    "%sql\n",
    "drop table telecom_catalog_assign.landing_zone.dlt_tb1\n",
    "\n",
    "##Use spark.read.sql to write some simple queries on the above tables created.\n",
    "df=spark.sql(\"select * from telecom_catalog_assign.landing_zone.dlt_tb\")\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b841905-e086-46cf-99ee-803da7a2bc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f6606f-3fa9-41a9-bb89-13a75ded8b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: Write customer data using insertInto() in a new table and find the behavior\n",
    "#table created for customer\n",
    "read_customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", sep=\",\").toDF(\"id\",\"name\",\"age\",\"city\",\"sim_type\") \n",
    "read_customer_df.write.saveAsTable(\"telecom_catalog_assign.landing_zone.custtbl\",mode='overwrite')\n",
    "#insert into customer table already created\n",
    "dlt_tb = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"id\", \"name\", \"age\", \"city\", \"sim_type\")\n",
    "dlt_tb.write.insertInto(\"telecom_catalog_assign.landing_zone.custtbl\")\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.custtbl\"))\n",
    "\n",
    "#2:Write usage data using insertTable() with overwrite mode\n",
    "read_usage = spark.read.options(header=\"True\",sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_new.csv\") \n",
    "read_usage.write.insertInto(\"telecom_catalog_assign.landing_zone.usagetbl\",overwrite=True)\n",
    "spark.sql(\"select * from telecom_catalog_assign.landing_zone.usagetbl\").show()\n",
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94f11454-e561-4108-b954-b023d93dc432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff34bc6-9e3f-4140-90e7-0e7dcbce69da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: Write customer data into XML format using rowTag as cust\n",
    "xml_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"id\", \"name\", \"age\", \"city\", \"sim_type\")\n",
    "xml_cust.write.format(\"xml\").mode(\"overwrite\").option(\"rowTag\", \"customer\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/xmlout\")\n",
    "\n",
    "#2: Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "read_usage = spark.read.options(header=\"True\",sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_new.csv\") \n",
    "read_usage.write.format(\"xml\").mode(\"overwrite\").option(\"rowTag\", \"usage\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/xmlout\") \n",
    "\n",
    "#3: Download the xml data and open the file in notepad++ and see how the xml file looks like.\n",
    "#Yes, we can see the xml file in notepad++ and it looks like HTML with the row starting and ending with the values provided in rowtag.\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed756c3-01e9-4a47-8454-d23f669762e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3aaf39a-a614-499c-aefa-c24a702e0cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "|Format  | File Type                | Memory size     |Human Readable Supported |\n",
    "|--------|--------------------------|------------------|------------------|\n",
    "| CSV    | Structured               |    9.8MB    | Yes               | \n",
    "| JSON   | Semi-structured           |   20.5MB      | Yes               | \n",
    "| ORC    | Structured / Striped      | 61.2kb            | NO                |  \n",
    "| Parquet| Structured / Nested       | 31 kb           | NO                |  \n",
    "| Delta  | Structured / Evolving     | 31kb           | No                |\n",
    "| XML    | Semi-structured           | 56.2 MB         | Yes               | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f4ec44-74a8-4b2a-999b-37e23c6bc23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like...\n",
    "1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85cbc9f1-cbf5-4606-8d53-bc690c72c365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "df1_orc = spark.read.format(\"orc\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout/part-00000-tid-9003774055315214043-348de395-2b2b-4f8c-a65c-957dfa381f7a-262-1-c000.snappy.orc\") \n",
    "#display(df1_orc) \n",
    "df1_orc.write.mode(\"overwrite\").parquet(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout/\")\n",
    "\n",
    "#2: Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "df1_parquet = spark.read.parquet(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout/part-00000-tid-1340631765587028736-838d0260-8279-4a2e-b51c-848e8cd2c955-266-1.c000.snappy.parquet\")\n",
    "#display(df1_parquet) \n",
    "df1_parquet.write.format(\"delta\").save(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout1/\")\n",
    "\n",
    "#3: Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "df1_delta = spark.read.format(\"delta\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout1/\")\n",
    "#display(df1_delta) \n",
    "df1_delta.write.format(\"xml\").mode(\"overwrite\").option(\"rowTag\", \"usage\").save(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/xmlout1/\")\n",
    "\n",
    "#4: Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "df1_delta = spark.read.format(\"delta\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout1/\")\n",
    "#display(df1_delta) \n",
    "df1_delta.write.format(\"json\").mode(\"overwrite\").save(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/jsonout1/\")\n",
    "\n",
    "#5: Read any one of the above delta table in a dataframe and write it to another table in the same database\n",
    "df1_delta = spark.read.format(\"delta\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout1/\")\n",
    "#display(df1_delta) \n",
    "df1_delta.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.landing_zone.usagetbl1\")\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.usagetbl1\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a5b5f7c-2e55-4ff3-bb56-140d1b2e896d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##16. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv \n",
    "-  benefits of CSV is simple data transfer/exchange and fast viewing because it is human-readable\n",
    "-  when we need small data analysis, and supported everywhere\n",
    "-  but it is not suitable for big-data processing because analytics is slow\n",
    "-  best for source data ingestion\n",
    "-  Easy for debugging\n",
    "-  Easy to create\n",
    "\n",
    "2. When to use/benifits json-\n",
    "-  JSON is semi-structured file type \n",
    "-  API integrations because it supports nested and flexible schemas, \n",
    "-  making it ideal for event and application data.\n",
    "-  it is a dictionary of dictionaries, best for data ingestion.\n",
    "-  widely supported.\n",
    "\n",
    "3. When to use/benifit orc \n",
    "- Use ORC for large data, \n",
    "- intelligent bigdata format, \n",
    "- read-heavy analytical workloads because it provides excellent compression, \n",
    "- fast reads, and efficient predicate pushdown.\n",
    "\n",
    "4. When to use/benifit parquet \n",
    "- Use Parquet for big-data analytics across multiple platforms because it is a columnar, compressed, \n",
    "- widely supported format that improves query performance.\n",
    "\n",
    "5. When to use/benifit delta-\n",
    "- Use Delta for reliable data lake processing because it adds ACID transactions, \n",
    "- schema enforcement, time travel. \n",
    "- scalable incremental loads on top of Parquet.\n",
    "\n",
    "6. When to use/benifit xml\n",
    "-  Use XML for legacy systems and structured data exchange where strict schemas \n",
    "-  hierarchical data representation are required.\n",
    "-  XML is safer when structure must not break.\n",
    "\n",
    "7. When to use/benifit delta tables\n",
    "- Use Delta tables for production-grade lakehouse architectures because they ensure data consistency, versioning, and optimized performance for large datasets."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "_read_write_usecases (3)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
